###EDUCATION 
#Bachelor of Engineering in Computer Engineering 
#Gujarat Technological University, Ahmedabad, Gujarat        Jan 2024 - Oct 2027 (Expected Graduation: October 2027) 
(Skills: Programming and Development Skills, Software Engineering and Testing Skills, Database Management Skills, 
Networking and Security Skills, Cloud Computing and Big Data Skills, Machine Learning and AI concepts, IoT (Internet of 
Things), Operating Systems and Computer Architecture Skills, Project Management and Ethics Skills, Problem-Solving 
and innovative thinking, Effective verbal and written Communication Skills, Teamwork and collaboration, Critical Thinking 
and Analytical Skills, Time Management and Organizational Skills) 
#Diploma of Engineering in Computer Engineering 
#Gujarat Technological University, Ahmedabad, Gujarat         Aug 2021 - May 2024 
(Skills and Technologies: Python, SQL and NoSQL Databases, Cloud Computing, IoT (Internet of Things), Mobile 
Application Development, Machine Learning and AI, Operating Systems Internals, Networking, Cybersecurity, Software 
Engineering, Computer Organizations and Architecture, etc)

###PROJECTS 
##Real-Time Stock Market Data Pipeline
o	Developed a comprehensive data engineering pipeline to ingest, process, and analyze real-time stock market data using Apache Kafka and AWS services.
o	Implemented a highly available Kafka cluster with three brokers for real-time data streaming, utilizing Python's `kafka-python` library to publish stock data from a REST API.
o	Architected a tiered S3 bucket structure with separate buckets for raw data (landing zone), processed data (staging zone), and aggregated data (reporting zone), optimizing for cost and performance.
o	Configured AWS Glue Crawler and Catalog to automatically discover and manage metadata for the S3 data lake, enabling efficient data querying and analysis.
o	Developed ETL jobs using AWS Glue to transform raw stock data, clean inconsistencies, and aggregate data into time-series formats suitable for analysis.
o	Utilized AWS Athena for running SQL queries on processed data, enabling real-time analysis of stock trends and identification of investment opportunities.
o	Successfully created a robust and scalable data pipeline that ingests and processes over 1 million stock data points per day with an average latency of 50 milliseconds, enabling real-time analysis and informed investment decisions. This pipeline reduced the time to insight from days to minutes, allowing for quicker responses to market fluctuations.

![Architecture](https://github.com/Kush-Bhatt-30/StockmarketAnalysis/blob/main/Architecture.jpg?raw=true)

#Technology Used:
-Programming Language: Python
-Amazon Web Service (AWS):
S3 (Simple Storage Service)
Athena
Glue Crawler
Glue Catalog
EC2
-Apache Kafka

[StockmarketAnalysis](https://github.com/Kush-Bhatt-30/StockmarketAnalysis/tree/main)

##Spotify Global Trending Songs ETL Pipeline 
o Built an ETL pipeline to fetch and process Spotify global trending songs using Spotify APIs, AWS 
services, and Snowflake. 
o Developed Python scripts to interact with Spotify APIs and fetch trending songs data, utilizing the 
`requests` library for API calls and handling pagination for large datasets.Utilized AWS Lambda for 
serverless data processing and AWS S3 for data storage. 
o Utilized AWS Lambda for serverless data processing, triggered by CloudWatch Events on a daily 
schedule, and configured with sufficient memory and execution time to handle the data volume. 
o Leveraged Snowflake for efficient data warehousing and Snowpipe for continuous data ingestion, creating 
a dedicated Snowflake database and schema for storing the Spotify data. 
o Implemented data transformation logic using SQL in Snowflake to clean and structure the data for 
analysis, including handling missing values, converting data types, and calculating song popularity 
metrics based on streaming counts and chart positions. 
o Successfully created a scalable ETL pipeline that reduced data processing time by 60% compared to 
manual methods and enabled real-time analysis of global music trends, facilitating the identification of 
emerging artists and trending genres. This pipeline powers a dashboard that provides insights into music 
consumption patterns across different countries and demographics. 

![architecture](https://github.com/Kush-Bhatt-30/Spotify_DE_Project/blob/main/Spotify_Project_KB_Architecture.jpg?raw=true)

#Technology Used:
-Programming Language: Python
-Cloud Services: AWS (Lambda, S3)
-Data Warehousing: Snowflake
-Integration Tools: Snowpipe

[Spotify_DE_Project](https://github.com/Kush-Bhatt-30/Spotify_DE_Project/tree/main)

###TECHNICAL SKILLS 
• Programming Languages: Python, Java, Scala, Spark, Shell, SQL 
• Data Engineering Tools: Spark, PySpark, Hadoop, Kafka, AWS Glue, Snowflake, Snowpipe 
• Databases: PostgreSQL, MongoDB, MS SQL, Oracle 
• Cloud Technologies: Amazon AWS, Microsoft Azure, Google Cloud Platform (GCP), Databricks 
• DevOps: Git, GitHub, Bitbucket, Docker, Kubernetes, Jenkins, Maven, Terraform

###CERTIFICATIONS 
AWS Certified Data Engineer - Associate, Amazon                                                                                         
Nov 2024 
(validates proficiency in designing and implementing data pipelines using AWS services, including expertise in data 
storage, transformation, and querying.) 
Databricks Certified Data Engineer Professional, Databricks                                                                               
Oct 2024 
(certifies advanced skills in building and managing scalable data pipelines using Apache Spark and Databricks, with a 
focus on optimizing performance and data reliability.) 
AWS Certified Cloud Practitioner, Amazon  
(demonstrates a comprehensive understanding of AWS cloud concepts, services, and best practices, validating 
foundational cloud knowledge and skills.) 
